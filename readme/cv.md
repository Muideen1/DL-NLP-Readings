# Computer Vision

> Including **Classification**, **Recognition**, **Detection**, **Captioning**, **GAN** and etc.

## Datasets
- [2018 CVPR] **Referring Relationships**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Krishna_Referring_Relationships_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Referring%20Relationships.bib), [[homepage]](https://cs.stanford.edu/people/ranjaykrishna/referringrelationships/), sources: [[StanfordVL/ReferringRelationships]](https://github.com/StanfordVL/ReferringRelationships).
- [2019 CVPR] **From Recognition to Cognition: Visual Commonsense Reasoning**, [[paper]](https://arxiv.org/pdf/1811.10830.pdf), [[bibtex]](/Bibtex/From%20Recognition%20to%20Cognition%20-%20Visual%20Commonsense%20Reasoning.bib), [[homepage]](https://visualcommonsense.com), [[leaderboard]](https://visualcommonsense.com/leaderboard/), [[dataset]](https://visualcommonsense.com/download/), sources: [[rowanz/r2c]](https://github.com/rowanz/r2c/).

## Video Localizing
- [2017 ICCV] **Localizing Moments in Video with Natural Language**, [[paper]](https://people.eecs.berkeley.edu/~lisa_anne/didemo/paper_arxiv.pdf), [[bibtex]](/Bibtex/Localizing%20Moments%20in%20Video%20with%20Natural%20Language.bib), sources: [[LisaAnne/LocalizingMoments]](https://github.com/LisaAnne/LocalizingMoments).
- [2017 ICCV] **TALL: Temporal Activity Localization via Language Query**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2017/papers/Gao_TALL_Temporal_Activity_ICCV_2017_paper.pdf), [[bibtex]](/Bibtex/TALL%20-%20Temporal%20Activity%20Localization%20via%20Language%20Query.bib), sources: [[jiyanggao/TALL]](https://github.com/jiyanggao/TALL).
- [2017 ICCV] **TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals**, [[paper]](https://arxiv.org/pdf/1703.06189.pdf), [[supplementary]](http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Gao_TURN_TAP_Temporal_ICCV_2017_supplemental.pdf), [[bibtex]](/Bibtex/TURN%20TAP%20-%20Temporal%20Unit%20Regression%20Network%20for%20Temporal%20Action%20Proposals.bib), sources: [[jiyanggao/TURN-TAP]](https://github.com/jiyanggao/TURN-TAP).
- [2018 EMNLP] **Localizing Moments in Video with Temporal Language**, [[paper]](https://aclweb.org/anthology/D18-1168), [[bibtex]](/Bibtex/Localizing%20Moments%20in%20Video%20with%20Temporal%20Language.bib), sources: [[LisaAnne/TemporalLanguageRelease]](https://github.com/LisaAnne/TemporalLanguageRelease).
- [2018 EMNLP] **Temporally Grounding Natural Sentence in Video**, [[paper]](https://www.aclweb.org/anthology/D18-1015), [[bibtex]](/Bibtex/Temporally%20Grounding%20Natural%20Sentence%20in%20Video.bib).
- [2018 ECCV] **Temporal Modular Networks for Retrieving Complex Compositional Activities in Videos**, [[paper]](http://openaccess.thecvf.com/content_ECCV_2018/papers/Bingbin_Liu_Temporal_Modular_Networks_ECCV_2018_paper.pdf), [[bibtex]](/Bibtex/Temporal%20Modular%20Networks%20for%20Retrieving%20Complex%20Compositional%20Activities%20in%20Videos.bib), [[homepage]](https://clarabing.github.io/tmn/).
- [2018 ArXiv] **Attentive Sequence to Sequence Translation for Localizing Clips of Interest by Natural Language Descriptions**, [[paper]](https://arxiv.org/pdf/1808.08803.pdf), [[bibtex]](/Bibtex/Attentive%20Sequence%20to%20Sequence%20Translation%20for%20Localizing%20Clips%20of%20Interest%20by%20Natural%20Language%20Descriptions.bib), sources: [[NeonKrypton/ASST]](https://github.com/NeonKrypton/ASST).
- [2018 SIGIR] **Attentive Moment Retrieval in Videos**, [[paper]](/Documents/Papers/Attentive%20Moment%20Retrieval%20in%20Videos.pdf), [[bibtex]](/Bibtex/Attentive%20Moment%20Retrieval%20in%20Videos.bib), [[slides]](https://pdfs.semanticscholar.org/5dc8/f69ad9404ed9e8d2318dca19f4eb534440a5.pdf), [[codes]](https://sigir2018.wixsite.com/acrn).
- [2018 AAAI] **To Find Where You Talk: Temporal Sentence Localization in Video with Attention Based Location Regression**, [[paper]](https://arxiv.org/pdf/1804.07014.pdf), [[bibtex]](/Bibtex/To%20Find%20Where%20You%20Talk%20-%20Temporal%20Sentence%20Localization%20in%20Video%20with%20Attention%20Based%20Location%20Regression.bib).
- [2019 WACV] **MAC: Mining Activity Concepts for Language-based Temporal Localization**, [[paper]](https://arxiv.org/pdf/1811.08925.pdf), [[bibtex]](/Bibtex/MAC%20-%20Mining%20Activity%20Concepts%20for%20Language-based%20Temporal%20Localization.bib), sources: [[runzhouge/MAC]](https://github.com/runzhouge/MAC).
- [2019 NAACL] **ExCL: Extractive Clip Localization Using Natural Language Descriptions**, [[paper]](https://www.aclweb.org/anthology/N19-1198), [[bibtex]](/Bibtex/ExCL%20-%20Extractive%20Clip%20Localization%20Using%20Natural%20Language%20Descriptions.bib).
- [2019 CVPR] **MAN: Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_MAN_Moment_Alignment_Network_for_Natural_Language_Moment_Retrieval_via_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/MAN%20-%20Moment%20Alignment%20Network%20for%20Natural%20Language%20Moment%20Retrieval%20via%20Iterative%20Graph%20Adjustment.bib).
- [2019 AAAI] **Localizing Natural Language in Videos**, [[paper]](http://forestlinma.com/welcome_files/Jingyuan_Chen_Localizing_Natural_Language_In_Videos_AAAI_2019.pdf), [[bibtex]](/Bibtex/Localizing%20Natural%20Language%20in%20Videos.bib).
- [2019 AAAI] **Multilevel Language and Vision Integration for Text-to-Clip Retrieval**, [[paper]](https://arxiv.org/pdf/1804.05113.pdf), [[bibtex]](/Bibtex/Multilevel%20Language%20and%20Vision%20Integration%20for%20Text-to-Clip%20Retrieval.bib), sources: [[VisionLearningGroup/Text-to-Clip_Retrieval]](https://github.com/VisionLearningGroup/Text-to-Clip_Retrieval).
- [2019 AAAI] **Read, Watch, and Move: Reinforcement Learning for Temporally Grounding Natural Language Descriptions in Videos**, [[paper]](https://arxiv.org/pdf/1901.06829.pdf), [[bibtex]](/Bibtex/Read%20Watch%20and%20Move%20-%20Reinforcement%20Learning%20for%20Temporally%20Grounding%20Natural%20Language%20Descriptions%20in%20Videos.bib).
- [2019 AAAI] **Semantic Proposal for Activity Localization in Videos via Sentence Query**, [[paper]](/Documents/Papers/Semantic%20Proposal%20for%20Activity%20Localization%20in%20Videos%20via%20Sentence%20Query.pdf), [[bibtex]](/Bibtex/Semantic%20Proposal%20for%20Activity%20Localization%20in%20Videos%20via%20Sentence%20Query.bib).
- [2019 ArXiv] **Tripping through time: Efficient Localization of Activities in Videos**, [[paper]](https://arxiv.org/pdf/1904.09936.pdf), [[bibtex]](/Bibtex/Tripping%20through%20time%20-%20Efficient%20Localization%20of%20Activities%20in%20Videos.bib).


## Image Processing
- [2018 IJCAI] **DehazeGAN: When Image Dehazing Meets Differential Programming**, [[paper]](http://www.ijcai.org/proceedings/2018/0172.pdf).
- [2018 AAAI] **Towards Perceptual Image Dehazing by Physics-based Disentanglement and Adversarial Training**, [[paper]](https://pdfs.semanticscholar.org/7a73/6b46b37a67a440a29593e261f7c0b63f0ad5.pdf).
- [2019 ArXiv] **EnlightenGAN: Deep Light Enhancement without Paired Supervision**, [[paper]](https://arxiv.org/pdf/1906.06972.pdf), [[bibtex]](/Bibtex/EnlightenGAN%20-%20Deep%20Light%20Enhancement%20without%20Paired%20Supervision.bib), sources: [[TAMU-VITA/EnlightenGAN]](https://github.com/TAMU-VITA/EnlightenGAN).

## Image Classification/Recognition and Object Detection
- [2012 NIPS] **ImageNet Classification with Deep Convolutional Neural Networks**, _AlexNet_, [[paper]](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).
- [2014 ArXiv] **Very Deep Convolutional Networks for Large-Scale Image Recognition**, _VGG_, [[paper]](https://arxiv.org/abs/1409.1556.pdf).
- [2015 CVPR] **Going Deeper with Convolutions**, _GoogLeNet_, [[paper]](https://arxiv.org/abs/1409.4842.pdf).
- [2015 ICCV] **Learning Spatiotemporal Features with 3D Convolutional Networks**, [[paper]](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf), [[bibtex]](/Bibtex/Learning%20Spatiotemporal%20Features%20with%203D%20Convolutional%20Networks.bib), [[homepage]](http://vlg.cs.dartmouth.edu/c3d/), sources: [[facebook/C3D]](https://github.com/facebook/C3D), [[hx173149/C3D-tensorflow]](https://github.com/hx173149/C3D-tensorflow).
- [2015 ICML] **Siamese Neural Networks for One-shot Image Recognition**, [[paper]](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf), sources: [[Goldesel23/Siamese-Networks-for-One-Shot-Learning]](https://github.com/Goldesel23/Siamese-Networks-for-One-Shot-Learning).
- [2016 CVPR] **Deep Residual Learning for Image Recognition**, [[paper]](https://arxiv.org/abs/1512.03385), sources: [[IsaacChanghau/AmusingPythonCodes/resnet]](https://github.com/IsaacChanghau/AmusingPythonCodes/tree/master/resnet), [[wenxinxu/resnet-in-tensorflow]](https://github.com/wenxinxu/resnet-in-tensorflow).
- [2017 CVPR] **Feature Pyramid Networks for Object Detection**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf), [[bibtex]](/Bibtex/Feature%20Pyramid%20Networks%20for%20Object%20Detection.bib), sources: [[unsky/FPN]](https://github.com/unsky/FPN), [[DetectionTeamUCAS/FPN_Tensorflow]](https://github.com/DetectionTeamUCAS/FPN_Tensorflow), [[yangxue0827/FPN_Tensorflow]](https://github.com/yangxue0827/FPN_Tensorflow).
- [2017 ICCV] **Mask R-CNN**, [[paper]](https://arxiv.org/pdf/1703.06870.pdf), [[tutorial]](http://kaiminghe.com/iccv17tutorial/maskrcnn_iccv2017_tutorial_kaiminghe.pdf), [[video]](https://www.youtube.com/watch?v=2TikTv6PWDw), sources: [[matterport/Mask_RCNN]](https://github.com/matterport/Mask_RCNN), [[CharlesShang/FastMaskRCNN]](https://github.com/CharlesShang/FastMaskRCNN), [[facebookresearch/Detectron]](https://github.com/facebookresearch/Detectron).
- [2017 ICCV] **Focal Loss for Dense Object Detection**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf), [[bibtex]](/Bibtex/Focal%20Loss%20for%20Dense%20Object%20Detection.bib), sources: [[unsky/focal-loss]](https://github.com/unsky/focal-loss), [[ailias/Focal-Loss-implement-on-Tensorflow]](https://github.com/ailias/Focal-Loss-implement-on-Tensorflow).
- [2017 CVPR] **Densely Connected Convolutional Networks**, [[paper]](https://arxiv.org/abs/1608.06993.pdf), sources: [[liuzhuang13/DenseNet]](https://github.com/liuzhuang13/DenseNet).
- [2017 CVPR] **Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset**, [[paper]](https://arxiv.org/pdf/1705.07750.pdf), [[bibtex]](/Bibtex/Quo%20Vadis%20Action%20Recognition%20A%20New%20Model%20and%20the%20Kinetics%20Dataset.bib), sources: [[deepmind/kinetics-i3d]](https://github.com/deepmind/kinetics-i3d), [[piergiaj/pytorch-i3d]](https://github.com/piergiaj/pytorch-i3d), [[hassony2/kinetics_i3d_pytorch]](https://github.com/hassony2/kinetics_i3d_pytorch), [[LossNAN/I3D-Tensorflow]](https://github.com/LossNAN/I3D-Tensorflow), [[tomrunia/PyTorchConv3D]](https://github.com/tomrunia/PyTorchConv3D), [[dlpbc/keras-kinetics-i3d]](https://github.com/dlpbc/keras-kinetics-i3d).
- [2018 CVPR] **Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs**, [[paper]](https://arxiv.org/pdf/1803.08035.pdf), [[blog]](https://www.cnblogs.com/wangxiaocvpr/p/8682608.html), sources: [[JudyYe/zero-shot-gcn]](https://github.com/JudyYe/zero-shot-gcn).
- [2018 ECCV] **DOCK: Detecting Objects by Transferring Common-sense Knowledge**, [[paper]](http://openaccess.thecvf.com/content_ECCV_2018/papers/Krishna_Kumar_Singh_Transferring_Common-Sense_Knowledge_ECCV_2018_paper.pdf), [[bibtex]](/Bibtex/DOCK%20-%20Detecting%20Objects%20by%20transferring%20Common-sense%20Knowledge.bib), sources: [[kkanshul/dock]](https://github.com/kkanshul/dock).
- [2019 ICML] **EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks**, [[paper]](http://proceedings.mlr.press/v97/tan19a/tan19a.pdf), [[bibtex]](/Bibtex/EfficientNet%20-%20Rethinking%20Model%20Scaling%20for%20Convolutional%20Neural%20Networks.bib), sources: [[tensorflow/tpu/models/official/efficientnet/]](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet), [[zsef123/EfficientNets-PyTorch]](https://github.com/zsef123/EfficientNets-PyTorch), [[narumiruna/efficientnet-pytorch]](https://github.com/narumiruna/efficientnet-pytorch), [[titu1994/keras-efficientnets]](https://github.com/titu1994/keras-efficientnets).

## Instance/Semantic Description/Segmentation
- [2015 IPMI] **Predicting Semantic Descriptions from Medical Images with Convolutional Neural Networks**, [[paper]](/Documents/Papers/Predicting%20Semantic%20Descriptions%20from%20Medical%20Images%20with%20Convolutional%20Neural%20Networks.pdf), [[bibtex]](/Bibtex/Predicting%20Semantic%20Descriptions%20from%20Medical%20Images%20with%20Convolutional%20Neural%20Networks.bib).
- [2017 CVPR] **PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation**, [[paper]](https://arxiv.org/pdf/1612.00593.pdf), sources: [[charlesq34/pointnet]](https://github.com/charlesq34/pointnet).
- [2018 ICML] **Attention-based Deep Multiple Instance Learning**, [[paper]](https://arxiv.org/pdf/1802.04712.pdf), [[bibtex]](/Bibtex/Attention-based%20Deep%20Multiple%20Instance%20Learning.bib), sources: [[AMLab-Amsterdam/AttentionDeepMIL]](https://github.com/AMLab-Amsterdam/AttentionDeepMIL).

## Image/Video Captioning
- [2015 ICML] **Show, Attend and Tell: Neural Image Caption Generation with Visual Attention**, [[paper]](https://arxiv.org/pdf/1502.03044.pdf), [[slides]](http://www.cs.toronto.edu/~fidler/slides/2017/CSC2539/Katherine_slides.pdf), [[bibtex]](/Bibtex/Neural%20Image%20Caption%20Generation%20with%20Visual%20Attention.bib),  [[homepage]](http://kelvinxu.github.io/projects/capgen.html), sources: [[kelvinxu/arctic-captions]](https://github.com/kelvinxu/arctic-captions), [[yunjey/show-attend-and-tell]](https://github.com/yunjey/show-attend-and-tell), [[DeepRNN/image_captioning]](https://github.com/DeepRNN/image_captioning), [[coldmanck/show-attend-and-tell]](https://github.com/coldmanck/show-attend-and-tell).
- [2017 PAMI] **Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge**, [[paper]](https://arxiv.org/abs/1609.06647.pdf), sources: [[tensorflow/models/im2txt]](https://github.com/tensorflow/models/tree/master/research/im2txt).
- [2018 ACL] **Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning**, [[paper]](http://aclweb.org/anthology/P18-1238), [[bibtex]](/Bibtex/Conceptual%20Captions%20-%20A%20Cleaned%20Hypernymed%20Image%20Alt-text%20Dataset%20For%20Automatic%20Image%20Captioning.bib), [[homepage]](https://ai.google.com/research/ConceptualCaptions), sources: [[google-research-datasets/conceptual-captions]](https://github.com/google-research-datasets/conceptual-captions).
- [2018 ArXiv] **Auto-Encoding Scene Graphs for Image Captioning**, [[paper]](https://arxiv.org/pdf/1812.02378.pdf), [[bibtex]](/Bibtex/Auto-Encoding%20Scene%20Graphs%20for%20Image%20Captioning.bib).
- [2018 NeurIPS] **Partially-Supervised Image Captioning**, [[paper]](https://papers.nips.cc/paper/7458-partially-supervised-image-captioning.pdf), [[bibtex]](/Bibtex/Partially-Supervised%20Image%20Captioning.bib).

## Action Recognition
- [2016 CVPR] **Dynamic Image Networks for Action Recognition**, [[paper]](https://www.egavves.com/data/cvpr2016bilen.pdf), [[bibtex]](/Bibtex/Dynamic%20Image%20Networks%20for%20Action%20Recognition.bib), sources: [[hbilen/dynamic-image-nets]](https://github.com/hbilen/dynamic-image-nets).
- [2016 CVPR] **End-to-end Learning of Action Detection from Frame Glimpses in Videos**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2016/papers/Yeung_End-To-End_Learning_of_CVPR_2016_paper.pdf), sources: [[syyeung/frameglimpses]](https://github.com/syyeung/frameglimpses).
- [2017 PAMI] **Action Recognition with Dynamic Image Networks**, [[paper]](http://homepages.inf.ed.ac.uk/hbilen/assets/pdf/Bilen17a.pdf), [[bibtex]](/Bibtex/Action%20Recognition%20with%20Dynamic%20Image%20Networks.bib), sources: [[hbilen/dynamic-image-nets]](https://github.com/hbilen/dynamic-image-nets).

## Multi-tasks and Multi-model
- [2018 ICLR] **Beyond Shared Hierarchies: Deep Multitask Learning Through Soft Layer Ordering**, [[paper]](https://openreview.net/pdf?id=BkXmYfbAZ), [[bibtex]](/Bibtex/Beyond%20Shared%20Hierarchies%20-%20Deep%20Multitask%20Learning%20Through%20Soft%20Layer%20Ordering.bib).

## Unassorted Research Works
- [2014 ECCV] **Visualizing and Understanding Convolutional Networks**, [[paper]](https://arxiv.org/abs/1311.2901.pdf).
- [2016 NIPS] **Matching Networks for One Shot Learning**, [[paper]](https://arxiv.org/pdf/1606.04080.pdf), source: [[AntreasAntoniou/MatchingNetworks]](https://github.com/AntreasAntoniou/MatchingNetworks), [[BoyuanJiang/matching-networks-pytorch]](https://github.com/BoyuanJiang/matching-networks-pytorch).
- [2017 ICLR] **Adversarially Learned Inference**, [[paper]](https://openreview.net/pdf?id=B1ElR4cgg), [[bibtex]](/Bibtex/Adversarially%20Learned%20Inference.bib), sources: [[IshmaelBelghazi/ALI]](https://github.com/IshmaelBelghazi/ALI), [[otenim/ALI-Keras2]](https://github.com/otenim/ALI-Keras2), [[edgarriba/ali-pytorch]](https://github.com/edgarriba/ali-pytorch).
- [2018 CVPR] **Focal Visual-Text Attention for Visual Question Answering**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Liang_Focal_Visual-Text_Attention_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Focal%20Visual-Text%20Attention%20for%20Visual%20Question%20Answering.bib), sources: [[JunweiLiang/FVTA_memoryqa]](https://github.com/JunweiLiang/FVTA_memoryqa).
- [2019 TPAMI] **Focal Visual-Text Attention for Memex Question Answering**, [[paper]](http://llcao.net/paper/MemexQA_TPAMI.pdf), [[bibtex]](/Bibtex/Focal%20Visual-Text%20Attention%20for%20Memex%20Question%20Answering.bib), [[homepage]](https://memexqa.cs.cmu.edu), sources: [[JunweiLiang/FVTA_memoryqa]](https://github.com/JunweiLiang/FVTA_memoryqa).
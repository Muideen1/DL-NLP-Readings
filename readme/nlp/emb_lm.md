# Char/Word Embeddings and Basline Systems

> Including **Character Embeddings**, **Word Embeddings** and **Baseline Systems**.

## Character Embeddings
- [2016 AAAI] **Char2Vec: Character-Aware Neural Language Models**, [[paper]](https://arxiv.org/pdf/1508.06615.pdf), sources: [[carpedm20/lstm-char-cnn-tensorflow]](https://github.com/carpedm20/lstm-char-cnn-tensorflow), [[yoonkim/lstm-char-cnn]](https://github.com/yoonkim/lstm-char-cnn).

## Word Embeddings
- [2008 NIPS] **HLBL: A Scalable Hierarchical Distributed Language Model**, [[paper]](http://www.cs.toronto.edu/~fritz/absps/andriytree.pdf), [[wenjieguan/Log-bilinear-language-models]](https://github.com/wenjieguan/Log-bilinear-language-models).
- [2010 INTERSPEECH] **RNNLM: Recurrent neural network based language model**, [[paper]](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf), [[Ph.D. Thesis]](http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf), [[slides]](http://www.fit.vutbr.cz/~imikolov/rnnlm/google.pdf), sources: [[mspandit/rnnlm]](https://github.com/mspandit/rnnlm).
- [2013 NIPS] **Word2Vec: Distributed Representations of Words and Phrases and their Compositionality**, [[paper]](https://arxiv.org/pdf/1310.4546.pdf), [[word2vec explained]](https://arxiv.org/pdf/1402.3722.pdf), [[params explained]](https://arxiv.org/pdf/1411.2738.pdf), [[blog]](https://isaacchanghau.github.io/post/word2vec/), sources: [[word2vec]](https://code.google.com/archive/p/word2vec/), [[dav/word2vec]](https://github.com/dav/word2vec), [[yandex/faster-rnnlm]](https://github.com/yandex/faster-rnnlm), [[tf-word2vec]](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/word2vec), [[zake7749/word2vec-tutorial]](https://github.com/zake7749/word2vec-tutorial).
- [2013 CoNLL] **Better Word Representations with Recursive Neural Networks for Morphology**, [[paper]](https://nlp.stanford.edu/~lmthang/data/papers/conll13_morpho.pdf).
- [2014 ACL] **Word2Vecf: Dependency-Based Word Embeddings**, [[paper]](http://www.aclweb.org/anthology/P14-2050), [[blog]](https://isaacchanghau.github.io/post/word2vecf/), sources: [[Yoav Goldberg/word2vecf]](https://bitbucket.org/yoavgo/word2vecf), [[IsaacChanghau/Word2VecfJava]](https://github.com/IsaacChanghau/Word2VecfJava).
- [2014 EMNLP] **GloVe: Global Vectors for Word Representation**, [[paper]](https://nlp.stanford.edu/pubs/glove.pdf), [[homepage]](https://nlp.stanford.edu/projects/glove/), sources: [[stanfordnlp/GloVe]](https://github.com/stanfordnlp/GloVe).
- [2014 ICML] **Compositional Morphology for Word Representations and Language Modelling**, [[paper]](http://proceedings.mlr.press/v32/botha14.pdf), sources: [[thompsonb/comp-morph]](https://github.com/thompsonb/comp-morph), [[claravania/subword-lstm-lm]](https://github.com/claravania/subword-lstm-lm).
- [2015 ACL] **Hyperword: Improving Distributional Similarity with Lessons Learned from Word Embeddings**, [[paper]](http://www.aclweb.org/anthology/Q15-1016), sources: [[Omer Levy/hyperwords]](https://bitbucket.org/omerlevy/hyperwords).
- [2016 NAACL] **Counter-fitting Word Vectors to Linguistic Constraints**, [[paper]](http://aclweb.org/anthology/N16-1018), [[bibtex]](/Bibtex/Counter-fitting%20Word%20Vectors%20to%20Linguistic%20Constraints.bib), sources: [[nmrksic/counter-fitting]](https://github.com/nmrksic/counter-fitting).
- [2016 ICLR] **Exploring the Limits of Language Modeling**, [[paper]](https://arxiv.org/pdf/1602.02410.pdf), [[slides]](https://www.cs.toronto.edu/~duvenaud/courses/csc2541/slides/lipnet.pdf), sources: [[tensorflow/models/lm_1b]](https://github.com/tensorflow/models/tree/master/research/lm_1b).
- [2016 CoNLL] **Context2Vec: Learning Generic Context Embedding with Bidirectional LSTM**, [[paper]](http://www.aclweb.org/anthology/K16-1006), sources: [[orenmel/context2vec]](https://github.com/orenmel/context2vec).
- [2016 IEEE Intelligent Systems] **How to Generate a Good Word Embedding?**, [[paper]](https://arxiv.org/pdf/1507.05523.pdf), [[基于神经网络的词和文档语义向量表示方法研究]](https://arxiv.org/pdf/1611.05962.pdf), [[blog]](http://licstar.net/archives/620), sources: [[licstar/compare]](https://github.com/licstar/compare).
- [2016 ArXiv] **Linear Algebraic Structure of Word Senses, with Applications to Polysemy**, [[paper]](https://arxiv.org/pdf/1601.03764.pdf), [[slides]](https://pdfs.semanticscholar.org/d770/5adf01fc9791337ed17dd37236129ef3a0f4.pdf), sources: [[YingyuLiang/SemanticVector]](https://github.com/YingyuLiang/SemanticVector).
- [2017 ACL] **FastText: Enriching Word Vectors with Subword Information**, [[paper]](https://arxiv.org/pdf/1607.04606.pdf), sources: [[facebookresearch/fastText]](https://github.com/facebookresearch/fastText), [[salestock/fastText.py]](https://github.com/salestock/fastText.py).
- [2017 ArXiv] **Implicitly Incorporating Morphological Information into Word Embedding**, [[paper]](https://arxiv.org/pdf/1701.02481.pdf).
- [2017 AAAI] **Improving Word Embeddings with Convolutional Feature Learning and Subword Information**, [[paper]](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14724/14187), sources: [[ShelsonCao/IWE]](https://github.com/ShelsonCao/IWE).
- [2018 ICML] **Learning K-way D-dimensional Discrete Codes for Compact Embedding Representations**, [[paper]](https://arxiv.org/pdf/1806.09464.pdf), [supplementary](http://web.cs.ucla.edu/~yzsun/papers/2018_icml_KDCoding_supp.pdf), sources: [[chentingpc/kdcode-lm]](https://github.com/chentingpc/kdcode-lm).
- [2018 ICLR] **Compressing Word Embeddings via Deep Compositional Code Learning**, [[paper]](https://openreview.net/pdf?id=BJRZzFlRb), [[bibtex]](/Bibtex/Compressing%20Word%20Embeddings%20via%20Deep%20Compositional%20Code%20Learning.bib), sources: [[msobroza/compositional_code_learning]](https://github.com/msobroza/compositional_code_learning).
- [2018 ACL] **Baseline Needs More Love - On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms**, [[paper]](https://www.aclweb.org/anthology/P18-1041), [[bibtex]](/Bibtex/Baseline%20Needs%20More%20Love.bib), sources: [[dinghanshen/SWEM]](https://github.com/dinghanshen/SWEM).

## Language Modeling Systems (Baseline Systems)
- [2017 NIPS] **Learned in Translation: Contextualized Word Vectors**, [[paper]](https://arxiv.org/pdf/1708.00107.pdf), sources: [[salesforce/cove]](https://github.com/salesforce/cove).
- [2018 NAACL] **Deep contextualized word representations**, [[paper]](https://arxiv.org/pdf/1802.05365.pdf), [[homepage]](https://allennlp.org/elmo), sources: [[allenai/bilm-tf]](https://github.com/allenai/bilm-tf), [[HIT-SCIR/ELMoForManyLangs]](https://github.com/HIT-SCIR/ELMoForManyLangs). Some extended application: [[UKPLab/elmo-bilstm-cnn-crf]](https://github.com/UKPLab/elmo-bilstm-cnn-crf).
- [2018 ArXiv] **GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations**, [[paper]](https://arxiv.org/pdf/1806.05662.pdf), [[bibtex]](GLoMo%20-%20Unsupervisedly%20Learned%20Relational%20Graphs%20as%20Transferable%20Representations.bib).
- [2018 ArXiv] **Improving Language Understanding by Generative Pre-Training**, [[paper]](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), [[bibtex]](/Bibtex/Improving%20Language%20Understanding%20by%20Generative%20Pre-Training.bib), [[homepage]](https://blog.openai.com/language-unsupervised/), sources: [[openai/finetune-transformer-lm]](https://github.com/openai/finetune-transformer-lm).
- [2018 ArXiv] **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**, [[paper]](https://arxiv.org/pdf/1810.04805.pdf), [[bibtex]](/Bibtex/BERT%20-%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding.bib), sources: [[google-research/bert]](https://github.com/google-research/bert), [[huggingface/pytorch-pretrained-BERT]](https://github.com/huggingface/pytorch-pretrained-BERT). Some extended for application: [[macanv/BERT-BiLSTM-CRF-NER]](https://github.com/macanv/BERT-BiLSTM-CRF-NER). Some blog post: 
  - daiwk的BERT解读: [I](https://daiwk.github.io/posts/nlp-bert.html), [II](https://daiwk.github.io/posts/nlp-bert-code-annotated-framework.html), [III](https://daiwk.github.io/posts/nlp-bert-code-annotated-application.html), [IV](https://daiwk.github.io/posts/nlp-bert-code.html)
  - Dissecting BERT: [I](https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3), [II](https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f), [III](https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73)
  - The Illustrated Transformer: [EN](https://jalammar.github.io/illustrated-transformer/), [CN](https://zhuanlan.zhihu.com/p/54356280)
  - The Annotated Transformer: [EN](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
  - 从Word Embedding到BERT - NLP预训练技术发展史: [CN](https://zhuanlan.zhihu.com/p/49271699)
  - NLP三大特征抽取器(CNN/RNN/Transformer)比较: [CN](https://zhuanlan.zhihu.com/p/54743941)
- [2019 ArXiv] **Transformer-XL: Attentive Language Models beyond a Fixed-Length Context**, [[paper]](https://arxiv.org/pdf/1901.02860.pdf), [[bibtex]](/Bibtex/Transformer-XL%20-%20Attentive%20Language%20Models%20beyond%20a%20Fixed-Length%20Context.bib), [[post]](https://towardsdatascience.com/transformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924), sources: [[kimiyoung/transformer-xl]](https://github.com/kimiyoung/transformer-xl).
- [2019 ArXiv] **Cross-lingual Language Model Pretraining**, [[paper]](https://arxiv.org/pdf/1901.07291.pdf), [[bibtex]](/Bibtex/Cross-lingual%20Language%20Model%20Pretraining.bib), sources: [[facebookresearch/XLM]](https://github.com/facebookresearch/XLM).
- [2019 ArXiv] **Language Models are Unsupervised Multitask Learners**, [[paper]](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf), [[homepage]](https://blog.openai.com/better-language-models/), sources: [[openai/gpt-2]](https://github.com/openai/gpt-2).
- [2019 ICLR] **What Do You Learn from Context? Probing for Sentence Structure in Contextualized Word Representations**, [[paper]](https://openreview.net/pdf?id=SJzSgnRcKX), [[bibtex]](/Bibtex/What%20Do%20You%20Learn%20from%20Context%20Probing%20for%20Sentence%20Structure%20in%20Contextualized%20Word%20Representations.bib).
- [2019 ICML] **MASS: Masked Sequence to Sequence Pre-training for Language Generation**, [[paper]](https://arxiv.org/pdf/1905.02450.pdf), [[bibtex]](/Bibtex/MASS%20-%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20Language%20Generation.bib), sources: [[xutaatmicrosoftdotcom/MASS]](https://github.com/xutaatmicrosoftdotcom/MASS).
- [2019 ACL] **ERNIE: Enhanced Language Representation with Informative Entities**, [[paper]](https://arxiv.org/pdf/1905.07129.pdf), [[bibtex]](/Bibtex/ERNIE%20-%20Enhanced%20Language%20Representation%20with%20Informative%20Entities.bib), [[blog]](https://www.jiqizhixin.com/articles/2019-05-26-4), sources: [[thunlp/ERNIE]](https://github.com/thunlp/ERNIE).
- [2019 ArXiv] **Cloze-driven Pretraining of Self-attention Networks**, [[paper]](https://arxiv.org/pdf/1903.07785.pdf), [[bibtex]](/Bibtex/Cloze-driven%20Pretraining%20of%20Self-attention%20Networks.bib).
- [2019 ArXiv] **XLNet: Generalized Autoregressive Pretraining for Language Understanding**, [[paper]](https://arxiv.org/pdf/1906.08237.pdf), [[bibtex]](/Bibtex/XLNet%20-%20Generalized%20Autoregressive%20Pretraining%20for%20Language%20Understanding.bib), sources: [[zihangdai/xlnet]](https://github.com/zihangdai/xlnet).
- [2019 ArXiv] **How multilingual is Multilingual BERT?**, [[paper]](https://arxiv.org/pdf/1906.01502.pdf), [[bibtex]](/Bibtex/How%20multilingual%20is%20Multilingual%20BERT.bib).

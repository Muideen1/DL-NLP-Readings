# Visual Grounding

## Image based Visual Grounding
### Image Captioning
- [2015 ICML] **Show, Attend and Tell: Neural Image Caption Generation with Visual Attention**, [[paper]](https://arxiv.org/pdf/1502.03044.pdf), [[slides]](http://www.cs.toronto.edu/~fidler/slides/2017/CSC2539/Katherine_slides.pdf), [[bibtex]](/Bibtex/Neural%20Image%20Caption%20Generation%20with%20Visual%20Attention.bib),  [[homepage]](http://kelvinxu.github.io/projects/capgen.html), sources: [[kelvinxu/arctic-captions]](https://github.com/kelvinxu/arctic-captions), [[yunjey/show-attend-and-tell]](https://github.com/yunjey/show-attend-and-tell), [[DeepRNN/image_captioning]](https://github.com/DeepRNN/image_captioning), [[coldmanck/show-attend-and-tell]](https://github.com/coldmanck/show-attend-and-tell).
- [2017 PAMI] **Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge**, [[paper]](https://arxiv.org/abs/1609.06647.pdf), sources: [[tensorflow/models/im2txt]](https://github.com/tensorflow/models/tree/master/research/im2txt).
- [2018 ACL] **Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning**, [[paper]](http://aclweb.org/anthology/P18-1238), [[bibtex]](/Bibtex/Conceptual%20Captions%20-%20A%20Cleaned%20Hypernymed%20Image%20Alt-text%20Dataset%20For%20Automatic%20Image%20Captioning.bib), [[homepage]](https://ai.google.com/research/ConceptualCaptions), sources: [[google-research-datasets/conceptual-captions]](https://github.com/google-research-datasets/conceptual-captions).
- [2018 ArXiv] **Auto-Encoding Scene Graphs for Image Captioning**, [[paper]](https://arxiv.org/pdf/1812.02378.pdf), [[bibtex]](/Bibtex/Auto-Encoding%20Scene%20Graphs%20for%20Image%20Captioning.bib).
- [2018 NeurIPS] **Partially-Supervised Image Captioning**, [[paper]](https://papers.nips.cc/paper/7458-partially-supervised-image-captioning.pdf), [[bibtex]](/Bibtex/Partially-Supervised%20Image%20Captioning.bib).

### Text-based Image Edit
- [2019 ACL] **Expressing Visual Relationships via Language**, [[paper]](https://www.aclweb.org/anthology/P19-1182), [[bibtex]](/Bibtex/Expressing%20Visual%20Relationships%20via%20Language.bib), sources: [[airsplay/VisualRelationships]](https://github.com/airsplay/VisualRelationships).

### Visual Question Answering
- [2016 NIPS] **Hierarchical Question-Image Co-Attention for Visual Question Answering**, [[paper]](https://arxiv.org/pdf/1606.00061), [[bibtex]](/Bibtex/Hierarchical%20Question-Image%20Co-Attention%20for%20Visual%20Question%20Answering.bib), sources: [[karunraju/VQA]](https://github.com/karunraju/VQA), [[jiasenlu/HieCoAttenVQA]](https://github.com/jiasenlu/HieCoAttenVQA).
- [2018 CVPR] **Visual Grounding via Accumulated Attention**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_Visual_Grounding_via_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Visual%20Grounding%20via%20Accumulated%20Attention.bib).


## Video based Visual Grounding
### Video Captioning
- [2015 ICCV] **Sequence to Sequence – Video to Text**, [[paper]](http://www.cs.utexas.edu/users/ml/papers/venugopalan.iccv15.pdf), [[bibtex]](/Bibtex/Sequence%20to%20Sequence%20–%20Video%20to%20Text.bib), [[homepage]](https://vsubhashini.github.io/s2vt.html), sources: [[vsubhashini/caffe/examples/s2vt]](https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt).
- [2017 ICCV] **Dense-Captioning Events in Videos**, [[paper]](https://arxiv.org/pdf/1705.00754.pdf), [[bibtex]](/Bibtex/Dense-Captioning%20Events%20in%20Videos.bib), [[homepage]](https://cs.stanford.edu/people/ranjaykrishna/densevid/), source: [[ranjaykrishna/densevid_eval]](https://github.com/ranjaykrishna/densevid_eval).
- [2018 CVPR] **Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning**, [[paper]](https://arxiv.org/pdf/1804.00100.pdf), [[bibtex]](/Bibtex/Bidirectional%20Attentive%20Fusion%20with%20Context%20Gating%20for%20Dense%20Video%20Captioning.bib), sources: [[JaywongWang/DenseVideoCaptioning]](https://github.com/JaywongWang/DenseVideoCaptioning).
- [2018 CVPR] **End-to-End Dense Video Captioning with Masked Transformer**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_End-to-End_Dense_Video_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/End-to-End%20Dense%20Video%20Captioning%20with%20Masked%20Transformer.bib), sources: [[salesforce/densecap]](https://github.com/salesforce/densecap).
- [2019 WACV] **Joint Event Detection and Description in Continuous Video Streams**, [[paper]](http://www.boyangli.co/paper/huijuanxu-wacv-2019.pdf), [[bibtex]](/Bibtex/Joint%20Event%20Detection%20and%20Description%20in%20Continuous%20Video%20Streams.bib), sources: [[VisionLearningGroup/JEDDi-Net]](https://github.com/VisionLearningGroup/JEDDi-Net).
- [2019 CVPR] **Grounded Video Description**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_Grounded_Video_Description_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Grounded%20Video%20Description.bib), sources: [[facebookresearch/ActivityNet-Entities]](https://github.com/facebookresearch/ActivityNet-Entities), [[facebookresearch/grounded-video-description]](https://github.com/facebookresearch/grounded-video-description).
- [2019 ArXiv] **Video Description: A Survey of Methods Datasets and Evaluation Metrics**, [[paper]](https://arxiv.org/pdf/1806.00186.pdf), [[bibtex]](/Bibtex/Video%20Description%20-%20A%20Survey%20of%20Methods%20Datasets%20and%20Evaluation%20Metrics.bib).

### Video Clip Localization
- [2017 ICCV] **Localizing Moments in Video with Natural Language**, [[paper]](https://people.eecs.berkeley.edu/~lisa_anne/didemo/paper_arxiv.pdf), [[bibtex]](/Bibtex/Localizing%20Moments%20in%20Video%20with%20Natural%20Language.bib), sources: [[LisaAnne/LocalizingMoments]](https://github.com/LisaAnne/LocalizingMoments).
- [2017 ICCV] **TALL: Temporal Activity Localization via Language Query**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2017/papers/Gao_TALL_Temporal_Activity_ICCV_2017_paper.pdf), [[bibtex]](/Bibtex/TALL%20-%20Temporal%20Activity%20Localization%20via%20Language%20Query.bib), sources: [[jiyanggao/TALL]](https://github.com/jiyanggao/TALL).
- [2017 ICCV] **TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals**, [[paper]](https://arxiv.org/pdf/1703.06189.pdf), [[supplementary]](http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Gao_TURN_TAP_Temporal_ICCV_2017_supplemental.pdf), [[bibtex]](/Bibtex/TURN%20TAP%20-%20Temporal%20Unit%20Regression%20Network%20for%20Temporal%20Action%20Proposals.bib), sources: [[jiyanggao/TURN-TAP]](https://github.com/jiyanggao/TURN-TAP).
- [2018 EMNLP] **Localizing Moments in Video with Temporal Language**, [[paper]](https://aclweb.org/anthology/D18-1168), [[bibtex]](/Bibtex/Localizing%20Moments%20in%20Video%20with%20Temporal%20Language.bib), sources: [[LisaAnne/TemporalLanguageRelease]](https://github.com/LisaAnne/TemporalLanguageRelease).
- [2018 EMNLP] **Temporally Grounding Natural Sentence in Video**, [[paper]](https://www.aclweb.org/anthology/D18-1015), [[bibtex]](/Bibtex/Temporally%20Grounding%20Natural%20Sentence%20in%20Video.bib).
- [2018 ECCV] **Temporal Modular Networks for Retrieving Complex Compositional Activities in Videos**, [[paper]](http://openaccess.thecvf.com/content_ECCV_2018/papers/Bingbin_Liu_Temporal_Modular_Networks_ECCV_2018_paper.pdf), [[bibtex]](/Bibtex/Temporal%20Modular%20Networks%20for%20Retrieving%20Complex%20Compositional%20Activities%20in%20Videos.bib), [[homepage]](https://clarabing.github.io/tmn/).
- [2018 ArXiv] **Attentive Sequence to Sequence Translation for Localizing Clips of Interest by Natural Language Descriptions**, [[paper]](https://arxiv.org/pdf/1808.08803.pdf), [[bibtex]](/Bibtex/Attentive%20Sequence%20to%20Sequence%20Translation%20for%20Localizing%20Clips%20of%20Interest%20by%20Natural%20Language%20Descriptions.bib), sources: [[NeonKrypton/ASST]](https://github.com/NeonKrypton/ASST).
- [2018 SIGIR] **Attentive Moment Retrieval in Videos**, [[paper]](/Documents/Papers/Attentive%20Moment%20Retrieval%20in%20Videos.pdf), [[bibtex]](/Bibtex/Attentive%20Moment%20Retrieval%20in%20Videos.bib), [[slides]](https://pdfs.semanticscholar.org/5dc8/f69ad9404ed9e8d2318dca19f4eb534440a5.pdf), [[codes]](https://sigir2018.wixsite.com/acrn).
- [2018 AAAI] **To Find Where You Talk: Temporal Sentence Localization in Video with Attention Based Location Regression**, [[paper]](https://arxiv.org/pdf/1804.07014.pdf), [[bibtex]](/Bibtex/To%20Find%20Where%20You%20Talk%20-%20Temporal%20Sentence%20Localization%20in%20Video%20with%20Attention%20Based%20Location%20Regression.bib).
- [2019 WACV] **MAC: Mining Activity Concepts for Language-based Temporal Localization**, [[paper]](https://arxiv.org/pdf/1811.08925.pdf), [[bibtex]](/Bibtex/MAC%20-%20Mining%20Activity%20Concepts%20for%20Language-based%20Temporal%20Localization.bib), sources: [[runzhouge/MAC]](https://github.com/runzhouge/MAC).
- [2019 NAACL] **ExCL: Extractive Clip Localization Using Natural Language Descriptions**, [[paper]](https://www.aclweb.org/anthology/N19-1198), [[bibtex]](/Bibtex/ExCL%20-%20Extractive%20Clip%20Localization%20Using%20Natural%20Language%20Descriptions.bib).
- [2019 CVPR] **MAN: Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_MAN_Moment_Alignment_Network_for_Natural_Language_Moment_Retrieval_via_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/MAN%20-%20Moment%20Alignment%20Network%20for%20Natural%20Language%20Moment%20Retrieval%20via%20Iterative%20Graph%20Adjustment.bib).
- [2019 AAAI] **Localizing Natural Language in Videos**, [[paper]](http://forestlinma.com/welcome_files/Jingyuan_Chen_Localizing_Natural_Language_In_Videos_AAAI_2019.pdf), [[bibtex]](/Bibtex/Localizing%20Natural%20Language%20in%20Videos.bib).
- [2019 AAAI] **Multilevel Language and Vision Integration for Text-to-Clip Retrieval**, [[paper]](https://arxiv.org/pdf/1804.05113.pdf), [[bibtex]](/Bibtex/Multilevel%20Language%20and%20Vision%20Integration%20for%20Text-to-Clip%20Retrieval.bib), sources: [[VisionLearningGroup/Text-to-Clip_Retrieval]](https://github.com/VisionLearningGroup/Text-to-Clip_Retrieval).
- [2019 AAAI] **Read, Watch, and Move: Reinforcement Learning for Temporally Grounding Natural Language Descriptions in Videos**, [[paper]](https://arxiv.org/pdf/1901.06829.pdf), [[bibtex]](/Bibtex/Read%20Watch%20and%20Move%20-%20Reinforcement%20Learning%20for%20Temporally%20Grounding%20Natural%20Language%20Descriptions%20in%20Videos.bib).
- [2019 AAAI] **Semantic Proposal for Activity Localization in Videos via Sentence Query**, [[paper]](/Documents/Papers/Semantic%20Proposal%20for%20Activity%20Localization%20in%20Videos%20via%20Sentence%20Query.pdf), [[bibtex]](/Bibtex/Semantic%20Proposal%20for%20Activity%20Localization%20in%20Videos%20via%20Sentence%20Query.bib).
- [2019 ArXiv] **Tripping through time: Efficient Localization of Activities in Videos**, [[paper]](https://arxiv.org/pdf/1904.09936.pdf), [[bibtex]](/Bibtex/Tripping%20through%20time%20-%20Efficient%20Localization%20of%20Activities%20in%20Videos.bib).

### Visual Question Answering
- [2018 CVPR] **Motion-Appearance Co-Memory Networks for Video Question Answering**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Gao_Motion-Appearance_Co-Memory_Networks_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Motion-Appearance%20Co-Memory%20Networks%20for%20Video%20Question%20Answering.bib).
- [2018 CVPR] **Focal Visual-Text Attention for Visual Question Answering**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Liang_Focal_Visual-Text_Attention_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Focal%20Visual-Text%20Attention%20for%20Visual%20Question%20Answering.bib), sources: [[JunweiLiang/FVTA_memoryqa]](https://github.com/JunweiLiang/FVTA_memoryqa).
- [2019 TPAMI] **Focal Visual-Text Attention for Memex Question Answering**, [[paper]](http://llcao.net/paper/MemexQA_TPAMI.pdf), [[bibtex]](/Bibtex/Focal%20Visual-Text%20Attention%20for%20Memex%20Question%20Answering.bib), [[homepage]](https://memexqa.cs.cmu.edu), sources: [[JunweiLiang/FVTA_memoryqa]](https://github.com/JunweiLiang/FVTA_memoryqa).
- [2019 ArXiv] **Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Fan_Heterogeneous_Memory_Enhanced_Multimodal_Attention_Model_for_Video_Question_Answering_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Heterogeneous%20Memory%20Enhanced%20Multimodal%20Attention%20Model%20for%20Video%20Question%20Answering.bib), [[poster]](http://homes.sice.indiana.edu/fan6/docs/cvpr19_videoqa.pdf), sources: [[fanchenyou/HME-VideoQA]](https://github.com/fanchenyou/HME-VideoQA).

### Video grounded dialogue



## Others
- [2018 CVPR] **Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Vision-and-Language%20Navigation.bib), sources: [[peteanderson80/Matterport3DSimulator]](https://github.com/peteanderson80/Matterport3DSimulator).
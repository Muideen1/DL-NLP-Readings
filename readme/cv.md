# Computer Vision

> Including **Classification**, **Recognition**, **Detection**, **Captioning**, **GAN** and etc.

## Datasets
- [2018 CVPR] **Referring Relationships**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Krishna_Referring_Relationships_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Referring%20Relationships.bib), [[homepage]](https://cs.stanford.edu/people/ranjaykrishna/referringrelationships/), sources: [[StanfordVL/ReferringRelationships]](https://github.com/StanfordVL/ReferringRelationships).
- [2019 CVPR] **From Recognition to Cognition: Visual Commonsense Reasoning**, [[paper]](https://arxiv.org/pdf/1811.10830.pdf), [[bibtex]](/Bibtex/From%20Recognition%20to%20Cognition%20-%20Visual%20Commonsense%20Reasoning.bib), [[homepage]](https://visualcommonsense.com), [[leaderboard]](https://visualcommonsense.com/leaderboard/), [[dataset]](https://visualcommonsense.com/download/), sources: [[rowanz/r2c]](https://github.com/rowanz/r2c/).

## Video Localizing
- [2017 ICCV] **Localizing Moments in Video with Natural Language**, [[paper]](https://people.eecs.berkeley.edu/~lisa_anne/didemo/paper_arxiv.pdf), [[bibtex]](/Bibtex/Localizing%20Moments%20in%20Video%20with%20Natural%20Language.bib), sources: [[LisaAnne/LocalizingMoments]](https://github.com/LisaAnne/LocalizingMoments).
- [2017 ICCV] **TALL: Temporal Activity Localization via Language Query**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2017/papers/Gao_TALL_Temporal_Activity_ICCV_2017_paper.pdf), [[bibtex]](/Bibtex/TALL%20-%20Temporal%20Activity%20Localization%20via%20Language%20Query.bib), sources: [[jiyanggao/TALL]](https://github.com/jiyanggao/TALL).
- [2018 EMNLP] **Localizing Moments in Video with Temporal Language**, [[paper]](https://aclweb.org/anthology/D18-1168), [[bibtex]](/Bibtex/Localizing%20Moments%20in%20Video%20with%20Temporal%20Language.bib), sources: [[LisaAnne/TemporalLanguageRelease]](https://github.com/LisaAnne/TemporalLanguageRelease).
- [2018 ArXiv] **Attentive Sequence to Sequence Translation for Localizing Clips of Interest by Natural Language Descriptions**, [[paper]](https://arxiv.org/pdf/1808.08803.pdf), [[bibtex]](/Bibtex/Attentive%20Sequence%20to%20Sequence%20Translation%20for%20Localizing%20Clips%20of%20Interest%20by%20Natural%20Language%20Descriptions.bib), sources: [[NeonKrypton/ASST]](https://github.com/NeonKrypton/ASST).


## Image Processing
- [2018 IJCAI] **DehazeGAN: When Image Dehazing Meets Differential Programming**, [[paper]](http://www.ijcai.org/proceedings/2018/0172.pdf).
- [2018 AAAI] **Towards Perceptual Image Dehazing by Physics-based Disentanglement and Adversarial Training**, [[paper]](https://pdfs.semanticscholar.org/7a73/6b46b37a67a440a29593e261f7c0b63f0ad5.pdf).

## Image Classification/Recognition and Object Detection
- [2012 NIPS] **ImageNet Classification with Deep Convolutional Neural Networks**, _AlexNet_, [[paper]](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).
- [2014 ArXiv] **Very Deep Convolutional Networks for Large-Scale Image Recognition**, _VGG_, [[paper]](https://arxiv.org/abs/1409.1556.pdf).
- [2015 CVPR] **Going Deeper with Convolutions**, _GoogLeNet_, [[paper]](https://arxiv.org/abs/1409.4842.pdf).
- [2015 ICCV] **Learning Spatiotemporal Features with 3D Convolutional Networks**, [[paper]](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf), [[bibtex]](/Bibtex/Learning%20Spatiotemporal%20Features%20with%203D%20Convolutional%20Networks.bib), [[homepage]](http://vlg.cs.dartmouth.edu/c3d/), sources: [[facebook/C3D]](https://github.com/facebook/C3D), [[hx173149/C3D-tensorflow]](https://github.com/hx173149/C3D-tensorflow).
- [2015 ICML] **Siamese Neural Networks for One-shot Image Recognition**, [[paper]](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf), sources: [[Goldesel23/Siamese-Networks-for-One-Shot-Learning]](https://github.com/Goldesel23/Siamese-Networks-for-One-Shot-Learning).
- [2016 CVPR] **Deep Residual Learning for Image Recognition**, [[paper]](https://arxiv.org/abs/1512.03385), sources: [[IsaacChanghau/AmusingPythonCodes/resnet]](https://github.com/IsaacChanghau/AmusingPythonCodes/tree/master/resnet), [[wenxinxu/resnet-in-tensorflow]](https://github.com/wenxinxu/resnet-in-tensorflow).
- [2017 CVPR] **Feature Pyramid Networks for Object Detection**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf), [[bibtex]](/Bibtex/Feature%20Pyramid%20Networks%20for%20Object%20Detection.bib), sources: [[unsky/FPN]](https://github.com/unsky/FPN), [[DetectionTeamUCAS/FPN_Tensorflow]](https://github.com/DetectionTeamUCAS/FPN_Tensorflow), [[yangxue0827/FPN_Tensorflow]](https://github.com/yangxue0827/FPN_Tensorflow).
- [2017 ICCV] **Mask R-CNN**, [[paper]](https://arxiv.org/pdf/1703.06870.pdf), [[tutorial]](http://kaiminghe.com/iccv17tutorial/maskrcnn_iccv2017_tutorial_kaiminghe.pdf), [[video]](https://www.youtube.com/watch?v=2TikTv6PWDw), sources: [[matterport/Mask_RCNN]](https://github.com/matterport/Mask_RCNN), [[CharlesShang/FastMaskRCNN]](https://github.com/CharlesShang/FastMaskRCNN), [[facebookresearch/Detectron]](https://github.com/facebookresearch/Detectron).
- [2017 ICCV] **Focal Loss for Dense Object Detection**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf), [[bibtex]](/Bibtex/Focal%20Loss%20for%20Dense%20Object%20Detection.bib), sources: [[unsky/focal-loss]](https://github.com/unsky/focal-loss), [[ailias/Focal-Loss-implement-on-Tensorflow]](https://github.com/ailias/Focal-Loss-implement-on-Tensorflow).
- [2017 CVPR] **Densely Connected Convolutional Networks**, [[paper]](https://arxiv.org/abs/1608.06993.pdf), sources: [[liuzhuang13/DenseNet]](https://github.com/liuzhuang13/DenseNet).
- [2018 CVPR] **Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs**, [[paper]](https://arxiv.org/pdf/1803.08035.pdf), [[blog]](https://www.cnblogs.com/wangxiaocvpr/p/8682608.html), sources: [[JudyYe/zero-shot-gcn]](https://github.com/JudyYe/zero-shot-gcn).
- [2018 ECCV] **DOCK: Detecting Objects by Transferring Common-sense Knowledge**, [[paper]](http://openaccess.thecvf.com/content_ECCV_2018/papers/Krishna_Kumar_Singh_Transferring_Common-Sense_Knowledge_ECCV_2018_paper.pdf), [[bibtex]](/Bibtex/DOCK%20-%20Detecting%20Objects%20by%20transferring%20Common-sense%20Knowledge.bib), sources: [[kkanshul/dock]](https://github.com/kkanshul/dock).

## Instance/Semantic Description/Segmentation
- [2015 IPMI] **Predicting Semantic Descriptions from Medical Images with Convolutional Neural Networks**, [[paper]](/Documents/Papers/Predicting%20Semantic%20Descriptions%20from%20Medical%20Images%20with%20Convolutional%20Neural%20Networks.pdf), [[bibtex]](/Bibtex/Predicting%20Semantic%20Descriptions%20from%20Medical%20Images%20with%20Convolutional%20Neural%20Networks.bib).
- [2017 CVPR] **PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation**, [[paper]](https://arxiv.org/pdf/1612.00593.pdf), sources: [[charlesq34/pointnet]](https://github.com/charlesq34/pointnet).
- [2018 ICML] **Attention-based Deep Multiple Instance Learning**, [[paper]](https://arxiv.org/pdf/1802.04712.pdf), [[bibtex]](/Bibtex/Attention-based%20Deep%20Multiple%20Instance%20Learning.bib), sources: [[AMLab-Amsterdam/AttentionDeepMIL]](https://github.com/AMLab-Amsterdam/AttentionDeepMIL).

## Image/Video Captioning
- [2015 ICML] **Show, Attend and Tell: Neural Image Caption Generation with Visual Attention**, [[paper]](https://arxiv.org/pdf/1502.03044.pdf), [[slides]](http://www.cs.toronto.edu/~fidler/slides/2017/CSC2539/Katherine_slides.pdf), [[bibtex]](/Bibtex/Neural%20Image%20Caption%20Generation%20with%20Visual%20Attention.bib),  [[homepage]](http://kelvinxu.github.io/projects/capgen.html), sources: [[kelvinxu/arctic-captions]](https://github.com/kelvinxu/arctic-captions), [[yunjey/show-attend-and-tell]](https://github.com/yunjey/show-attend-and-tell), [[DeepRNN/image_captioning]](https://github.com/DeepRNN/image_captioning), [[coldmanck/show-attend-and-tell]](https://github.com/coldmanck/show-attend-and-tell).
- [2017 PAMI] **Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge**, [[paper]](https://arxiv.org/abs/1609.06647.pdf), sources: [[tensorflow/models/im2txt]](https://github.com/tensorflow/models/tree/master/research/im2txt).
- [2018 ACL] **Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning**, [[paper]](http://aclweb.org/anthology/P18-1238), [[bibtex]](/Bibtex/Conceptual%20Captions%20-%20A%20Cleaned%20Hypernymed%20Image%20Alt-text%20Dataset%20For%20Automatic%20Image%20Captioning.bib), [[homepage]](https://ai.google.com/research/ConceptualCaptions), sources: [[google-research-datasets/conceptual-captions]](https://github.com/google-research-datasets/conceptual-captions).
- [2018 ArXiv] **Auto-Encoding Scene Graphs for Image Captioning**, [[paper]](https://arxiv.org/pdf/1812.02378.pdf), [[bibtex]](/Bibtex/Auto-Encoding%20Scene%20Graphs%20for%20Image%20Captioning.bib).
- [2018 NeurIPS] **Partially-Supervised Image Captioning**, [[paper]](https://papers.nips.cc/paper/7458-partially-supervised-image-captioning.pdf), [[bibtex]](/Bibtex/Partially-Supervised%20Image%20Captioning.bib).

## Action Recognition
- [2016 CVPR] **Dynamic Image Networks for Action Recognition**, [[paper]](https://www.egavves.com/data/cvpr2016bilen.pdf), [[bibtex]](/Bibtex/Dynamic%20Image%20Networks%20for%20Action%20Recognition.bib), sources: [[hbilen/dynamic-image-nets]](https://github.com/hbilen/dynamic-image-nets).
- [2016 CVPR] **End-to-end Learning of Action Detection from Frame Glimpses in Videos**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2016/papers/Yeung_End-To-End_Learning_of_CVPR_2016_paper.pdf), sources: [[syyeung/frameglimpses]](https://github.com/syyeung/frameglimpses).
- [2017 PAMI] **Action Recognition with Dynamic Image Networks**, [[paper]](http://homepages.inf.ed.ac.uk/hbilen/assets/pdf/Bilen17a.pdf), [[bibtex]](/Bibtex/Action%20Recognition%20with%20Dynamic%20Image%20Networks.bib), sources: [[hbilen/dynamic-image-nets]](https://github.com/hbilen/dynamic-image-nets).

## Multi-tasks and Multi-model
- [2018 ICLR] **Beyond Shared Hierarchies: Deep Multitask Learning Through Soft Layer Ordering**, [[paper]](https://openreview.net/pdf?id=BkXmYfbAZ), [[bibtex]](/Bibtex/Beyond%20Shared%20Hierarchies%20-%20Deep%20Multitask%20Learning%20Through%20Soft%20Layer%20Ordering.bib).

## Unassorted Research Works
- [2014 ECCV] **Visualizing and Understanding Convolutional Networks**, [[paper]](https://arxiv.org/abs/1311.2901.pdf).
- [2016 NIPS] **Matching Networks for One Shot Learning**, [[paper]](https://arxiv.org/pdf/1606.04080.pdf), source: [[AntreasAntoniou/MatchingNetworks]](https://github.com/AntreasAntoniou/MatchingNetworks), [[BoyuanJiang/matching-networks-pytorch]](https://github.com/BoyuanJiang/matching-networks-pytorch).
- [2017 ICLR] **Adversarially Learned Inference**, [[paper]](https://openreview.net/pdf?id=B1ElR4cgg), [[bibtex]](/Bibtex/Adversarially%20Learned%20Inference.bib), sources: [[IshmaelBelghazi/ALI]](https://github.com/IshmaelBelghazi/ALI), [[otenim/ALI-Keras2]](https://github.com/otenim/ALI-Keras2), [[edgarriba/ali-pytorch]](https://github.com/edgarriba/ali-pytorch).
- [2018 CVPR] **Focal Visual-Text Attention for Visual Question Answering**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Liang_Focal_Visual-Text_Attention_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Focal%20Visual-Text%20Attention%20for%20Visual%20Question%20Answering.bib), sources: [[JunweiLiang/FVTA_memoryqa]](https://github.com/JunweiLiang/FVTA_memoryqa).
- [2019 TPAMI] **Focal Visual-Text Attention for Memex Question Answering**, [[paper]](http://llcao.net/paper/MemexQA_TPAMI.pdf), [[bibtex]](/Bibtex/Focal%20Visual-Text%20Attention%20for%20Memex%20Question%20Answering.bib), [[homepage]](https://memexqa.cs.cmu.edu), sources: [[JunweiLiang/FVTA_memoryqa]](https://github.com/JunweiLiang/FVTA_memoryqa).
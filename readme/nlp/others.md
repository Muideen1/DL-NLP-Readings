# Other NLP Research Works

> Including **NLP Survey**, **Optimizing Methods in NLP**, **Grammatical Error Correction**, **Code Generation**, **Recurrent Neural Network**, **Multi-task Learning** and etc.

## Natural Language Survey
- [2018 JAIR] **Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation**, [[paper]](https://arxiv.org/pdf/1703.09902.pdf).
- [2018 CIM] **Recent Trends in Deep Learning Based Natural Language Processing**, [[paper]](https://arxiv.org/pdf/1708.02709.pdf).

## Code Generation
- [2017 ESEC/FSE] **Are Deep Neural Networks the Best Choice for Modeling Source Code?**, [[paper]](http://web.cs.ucdavis.edu/~devanbu/isDLgood.pdf), [[bibtex]](/Bibtex/Are%20Deep%20Neural%20Networks%20the%20Best%20Choice%20for%20Modeling%20Source%20Code.bib).
- [2018 EMNLP] **Mapping Language to Code in Programmatic Context**, [[paper]](https://aclweb.org/anthology/D18-1192), [[bibtex]](/Bibtex/Mapping%20Language%20to%20Code%20in%20Programmatic%20Context.bib), sources: [[sriniiyer/concode]](https://github.com/sriniiyer/concode).
- [2019 ArXiv] **Maybe Deep Neural Networks are the Best Choice for Modeling Source Code**, [[paper]](https://arxiv.org/pdf/1903.05734.pdf), [[bibtex]](/Bibtex/Maybe%20Deep%20Neural%20Networks%20are%20the%20Best%20Choice%20for%20Modeling%20Source%20Code.bib), [[slides]](https://research.jetbrains.org/files/material/5ce3172d8cfcc.pdf), sources: [[mast-group/OpenVocabCodeNLM]](https://github.com/mast-group/OpenVocabCodeNLM).
- [2019 ArXiv] **Neural Networks for Modeling Source Code Edits**, [[paper]](https://arxiv.org/pdf/1904.02818.pdf), [[bibtex]](/Bibtex/Neural%20Networks%20for%20Modeling%20Source%20Code%20Edits.bib).

## Grammatical Error Correction
- [2014 CoNLL] **The CoNLL-2014 Shared Task on Grammatical Error Correction**, [[paper]](http://www.aclweb.org/anthology/W14-1701), [[bibtex]](/Bibtex/The%20CoNLL-2014%20Shared%20Task%20on%20Grammatical%20Error%20Correction.bib) [[homepage]](http://www.comp.nus.edu.sg/~nlp/conll14st.html).
- [2018 AAAI] **A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction**, [[paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17308/16137), [[bibtex]](/Bibtex/A%20Multilayer%20Convolutional%20Encoder-Decoder%20Neural%20Network%20for%20Grammatical%20Error%20Correction.bib), [[nusnlp/mlconvgec2018]](https://github.com/nusnlp/mlconvgec2018).
- [2019 NAACL] **Improving Grammatical Error Correction via Pre-Training aCopy-Augmented Architecture with Unlabeled Data**, [[paper]](https://www.aclweb.org/anthology/N19-1014), [[bibtex]](/Bibtex/Improving%20Grammatical%20Error%20Correction%20via%20Pre-Training%20aCopy-Augmented%20Architecture%20with%20Unlabeled%20Data.bib), sources: [[zhawe01/fairseq-gec]](https://github.com/zhawe01/fairseq-gec).

## Multi-task Learning
- [2011 JMLR] **Natural Language Processing (Almost) from Scratch**, cover _Tagging, Chunking, Parsing, NER, SRL and etc._ tasks, [[paper]](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf), [[bibtex]](/Bibtex/Natural%20Language%20Processing%20%28Almost%29%20from%20Scratch.bib), sources: [[attardi/deepnl]](https://github.com/attardi/deepnl).
- [2017 ArXiv] **An Overview of Multi-Task Learning in Deep Neural Networks**, [[paper]](https://arxiv.org/pdf/1706.05098.pdf), [[bibtex]](/Bibtex/An%20Overview%20of%20Multi-Task%20Learning%20in%20Deep%20Neural%20Networks.bib).
- [2017 EMNLP] **A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks**, cover _Tagging, Chunking, Parsing, Relatedness, Entailment_ tasks, [[paper]](http://aclweb.org/anthology/D17-1206), [[bibtex]](/Bibtex/A%20Joint%20Many-Task%20Model%20-%20Growing%20a%20Neural%20Network%20for%20Multiple%20NLP%20Tasks.bib), [[blog]](https://theneuralperspective.com/2017/03/08/a-joint-many-task-model-growing-a-neural-network-for-multiple-nlp-tasks/), sources: [[rubythonode/joint-many-task-model]](https://github.com/rubythonode/joint-many-task-model), [[hassyGo/charNgram2vec]](https://github.com/hassyGo/charNgram2vec).
- [2019 AAAI] **Latent Multi-task Architecture Learning** or **Sluice Networks: Learning What to Share Between Loosely Related Tasks**, [[paper]](https://www.aaai.org/Papers/AAAI/2019/AAAI-RuderS.6318.pdf), [[bibtex]](/Bibtex/Latent%20Multi-task%20Architecture%20Learning.bib), sources: [[sebastianruder/sluice-networks]](https://github.com/sebastianruder/sluice-networks).

## Recurrent Neural Network
- [2001 PhD Thesis] **Long Short-Term Memory in Recurrent Neural Networks**, [[Gers' Ph.D. Thesis]](https://www.researchgate.net/profile/Felix_Gers/publication/2562741_Long_Short-Term_Memory_in_Recurrent_Neural_Networks/links/5759410a08ae9a9c954e77f5.pdf).
- [2014 ArXiv] **Recurrent Neural Network Regularization**, [[paper]](https://arxiv.org/abs/1409.2329).
- [2015 ArXiv] **Grid Long Short-Term Memory**, [[paper]](https://arxiv.org/abs/1507.01526), sources: [[Tensotflow-GridLSTMCell]](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/GridLSTMCell).
- [2016 ArXiv] **Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks**, [[paper]](https://arxiv.org/abs/1611.06204).
- [2016 ArXiv] **Contextual LSTM (CLSTM) models for Large scale NLP tasks**, [[paper]](https://arxiv.org/pdf/1602.06291v2.pdf), [[bibtex]](/Bibtex/Contextual%20LSTM%20models%20for%20Large%20scale%20NLP%20tasks.bib), sources: [[kafkasl/contextualLSTM]](https://github.com/kafkasl/contextualLSTM).
- [2016 ICLR] **Visualizing and Understanding Recurrent Networks**, [[paper]](http://vision.stanford.edu/pdf/KarpathyICLR2016.pdf).
- **Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences**, [[paper]](https://arxiv.org/pdf/1610.09513v1.pdf), sources: [[Tensorflow-PhasedLSTMCell]](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/PhasedLSTMCell).
- [2016 NIPS] **A Theoretically Grounded Application of Dropout in Recurrent Neural Networks**, [[paper]](https://arxiv.org/pdf/1512.05287.pdf), [[bibtex]](/Bibtex/A%20Theoretically%20Grounded%20Application%20of%20Dropout%20in%20Recurrent%20Neural%20Networks.bib), sources: [[DropoutWrapper TF]](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/DropoutWrapper).
- [2017 ACML] **Nested LSTMs**, [[paper]](https://arxiv.org/abs/1801.10308), sources: [[hannw/nlstm]](https://github.com/hannw/nlstm), [[titu1994/Nested-LSTM]](https://github.com/titu1994/Nested-LSTM).
- [2017 ICLR] **Variable Computation in Recurrent Neural Networks**, [[paper]](https://arxiv.org/pdf/1611.06188.pdf).
- [2018 EMNLP] **Simple Recurrent Units for Highly Parallelizable Recurrence**, [[paper]](http://aclweb.org/anthology/D18-1477), [[bibtex]](/Bibtex/Simple%20Recurrent%20Units%20for%20Highly%20Parallelizable%20Recurrence.bib), sources: [[taolei87/sru]](https://github.com/taolei87/sru).
- [2018 ICLR] **Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks**, [[paper]](https://arxiv.org/pdf/1708.06834.pdf), [[homepage]](https://imatge-upc.github.io/skiprnn-2017-telecombcn/), sources: [[imatge-upc/skiprnn-2017-telecombcn]](https://github.com/imatge-upc/skiprnn-2017-telecombcn).

## Others
- [2016 EMNLP] **How Transferable are Neural Networks in NLP Applications?**, [[paper]](http://www.aclweb.org/anthology/D16-1046).
- [2017 CIKM] **Commonsense for Machine Intelligence: Text to Knowledge and Knowledge to Text**, [[slides]](http://people.mpi-inf.mpg.de/~ntandon/presentations/cikm-2017-tutorial-commonsense/commonsense.pdf), [[CIKM 2017 Singapore Tutorials]](http://cikm2017.org/tutorialmain.html), [[Commonsense for Machine Intelligence, Allen Institute, CIKM 2017 TUTORIAL]](http://allenai.org/tutorials/csk/), [[Allen Institute]](http://allenai.org/index.html).
- [2017 ICLR] **An Actor Critic Algorithm for Structured Prediction**, [[paper]](https://openreview.net/pdf?id=SJDaqqveg), [[bibtex]](/Bibtex/An%20Actor%20Critic%20Algorithm%20for%20Structured%20Prediction.bib), sources: [[rizar/actor-critic-public]](https://github.com/rizar/actor-critic-public).
- [2017 ACL] **Learning When to Skim and When to Read**, [[paper]](http://www.aclweb.org/anthology/W17-2631), [[blog]](https://einstein.ai/research/learning-when-to-skim-and-when-to-read).
- [2018 ArXiv] **Fast Directional Self-Attention Mechanism**, [[paper]](https://arxiv.org/pdf/1805.00912.pdf), [[bibtex]](/Bibtex/Fast%20Directional%20Self-Attention%20Mechanism.bib), sources: [[taoshen58/Fast-DiSA]](https://github.com/taoshen58/DiSAN/tree/master/Fast-DiSA).
- [2018 ICLR] **Regularizing and Optimizing LSTM Language Models**, [[paper]](https://openreview.net/pdf?id=SyyGPP0TZ), [[bibtex]](/Bibtex/Regularizing%20and%20Optimizing%20LSTM%20Language%20Models.bib), sources: [[salesforce/awd-lstm-lm]](https://github.com/salesforce/awd-lstm-lm), author page: [[Nitish Shirish Keskar]](https://keskarnitish.github.io).
- [2018 NAACL] **Improving Implicit Discourse Relation Classification by Modeling Inter-dependencies of Discourse Units in a Paragraph**, [[paper]](http://www.aclweb.org/anthology/N18-1013), [[bibtex]](/Bibtex/Improving%20Implicit%20Discourse%20Relation%20Classification%20by%20Modeling%20Inter-dependencies%29of%20Discourse%20Units%20in%20a%20Paragraph.bib), sources: [[ZeyuDai/paragraph_implicit_discourse_relations]](https://github.com/ZeyuDai/paragraph_implicit_discourse_relations).
- [2018 COLING] **CASCADE: Contextual Sarcasm Detection in Online Discussion Forums**, [[paper]](http://aclweb.org/anthology/C18-1156), [[bibtex]](/Bibtex/Contextual%20Sarcasm%20Detection%20in%20Online%20Discussion%20Forums.bib), sources: [[SenticNet/CASCADE]](https://github.com/SenticNet/CASCADE).
- [2019 ACL] **Choosing Transfer Languages for Cross-Lingual Learning**, [[paper]](https://www.aclweb.org/anthology/P19-1301), [[bibtex]](Choosing%20Transfer%20Languages%20for%20Cross-Lingual%20Learning.bib), sources: [[neulab/langrank]](https://github.com/neulab/langrank).
- [2019 NAACL] **Attention is not Explanation**, [[paper]](https://www.aclweb.org/anthology/N19-1357.pdf), [[bibtex]](/Bibtex/Attention%20is%20not%20Explanation.bib), sources: [[successar/AttentionExplanation]](https://github.com/successar/AttentionExplanation).
- [2019 EMNLP] **Attention is not not Explanation**, [[paper]](https://arxiv.org/pdf/1908.04626.pdf), [[bibtex]](/Bibtex/Attention%20is%20not%20not%20Explanation.bib), sources: [[sarahwie/attention]](https://github.com/sarahwie/attention).
- [2019 ACL] **Augmenting Neural Networks with First-order Logic**, [[paper]](https://www.aclweb.org/anthology/P19-1028.pdf), [[bibtex]](/Bibtex/Augmenting%20Neural%20Networks%20with%20First-order%20Logic.bib), sources: [[utahnlp/layer_augmentation]](https://github.com/utahnlp/layer_augmentation).

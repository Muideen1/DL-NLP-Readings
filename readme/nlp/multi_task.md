# Multi-tasks Learning

- [2011 JMLR] **Natural Language Processing (Almost) from Scratch**, cover _Tagging, Chunking, Parsing, NER, SRL and etc._ tasks, [[paper]](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf), [[bibtex]](/Bibtex/Natural%20Language%20Processing%20%28Almost%29%20from%20Scratch.bib), sources: [[attardi/deepnl]](https://github.com/attardi/deepnl).
- [2017 ArXiv] **An Overview of Multi-Task Learning in Deep Neural Networks**, [[paper]](https://arxiv.org/pdf/1706.05098.pdf), [[bibtex]](/Bibtex/An%20Overview%20of%20Multi-Task%20Learning%20in%20Deep%20Neural%20Networks.bib).
- [2017 EMNLP] **A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks**, cover _Tagging, Chunking, Parsing, Relatedness, Entailment_ tasks, [[paper]](http://aclweb.org/anthology/D17-1206), [[bibtex]](/Bibtex/A%20Joint%20Many-Task%20Model%20-%20Growing%20a%20Neural%20Network%20for%20Multiple%20NLP%20Tasks.bib), [[blog]](https://theneuralperspective.com/2017/03/08/a-joint-many-task-model-growing-a-neural-network-for-multiple-nlp-tasks/), sources: [[rubythonode/joint-many-task-model]](https://github.com/rubythonode/joint-many-task-model), [[hassyGo/charNgram2vec]](https://github.com/hassyGo/charNgram2vec).
- [2018 ICLR] **Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling**, [[paper]](https://openreview.net/pdf?id=H1cWzoxA-), [[bibtex]](/Bibtex/Bi-Directional%20Block%20Self-Attention%20for%20Fast%20and%20Memory-Efficient%20Sequence%20Modeling.bib), sources: [[taoshen58/BiBloSA]](https://github.com/taoshen58/BiBloSA).
- [2018 CoNLL] **Sequence Classification with Human Attention**, [[paper]](http://aclweb.org/anthology/K18-1030), [[bibtex]](/Bibtex/Sequence%20classification%20with%20human%20attention.bib), sources: [[coastalcph/Sequence_classification_with_human_attention]](https://github.com/coastalcph/Sequence_classification_with_human_attention).
- [2019 ACL] **Multi-Task Deep Neural Networks for Natural Language Understanding**, [[paper]](https://www.aclweb.org/anthology/P19-1441.pdf), [[bibtex]](/Bibtex/Multi-Task%20Deep%20Neural%20Networks%20for%20Natural%20Language%20Understanding.bib), sources: [[namisan/mt-dnn]](https://github.com/namisan/mt-dnn).
- [2019 ArXiv] **BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning**, [[paper]](https://arxiv.org/pdf/1902.02671.pdf), [[bibtex]](/Bibtex/BERT%20and%20PALs%20-%20Projected%20Attention%20Layers%20for%20Efficient%20Adaptation%20in%20Multi-Task%20Learning.bib).
- [2019 AAAI] **Latent Multi-task Architecture Learning** or **Sluice Networks: Learning What to Share Between Loosely Related Tasks**, [[paper]](https://www.aaai.org/Papers/AAAI/2019/AAAI-RuderS.6318.pdf), [[bibtex]](/Bibtex/Latent%20Multi-task%20Architecture%20Learning.bib), sources: [[sebastianruder/sluice-networks]](https://github.com/sebastianruder/sluice-networks).